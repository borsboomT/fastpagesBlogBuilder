{
  
    
        "post0": {
            "title": "Beginner Mistakes in Machine Learning",
            "content": ". So you&#39;ve decided to pick up machine learning. That&#39;s fantastic! It can be incredibly powerful, and open up a ton of opportunities. Before you get started, we should have a little talk about some of the most common mistakes that beginners make when learning machine learning. The biggest culprit of all: overfitting. . What is overfitting? . Using mathematical models to estimate different parameters and properties is nothing new. The concept is basically as old as math itself, and when used correctly it can be incredibly powerful (not to mention sexy.üòâ) That has created a huge influx of people that want to learn how to get into the field of machine learning and become data scientists themselves. . One of the biggest roadblocks that people tend to run into when they are first learning how to use machine learning models is to make a model as complex as possible, and fit their training data to within an inch of its life. This is called overfitting, and it occurs when a model is fitted so well to a particular subset of data that it doesn&#39;t work on any other data at all. There are specific training and testing protocols for avoiding this, such as K-Fold Cross Validation, and I will explore how that works in another post. Here we&#39;ll focus on how overfitting occurs, and what it has to do with the bias and variance of a model. . Bias and variance . Bias actually describes any systematic error that can be introduced when fitting a model to a dataset. Common sources of bias include: . Model Bias. Bias introduced by choosing a model that is ill fit for the application at hand. You&#39;ll never be able to fit data well if the model you&#39;ve chosen is simply wrong for what you&#39;re doing. This can be eliminated through thoughtful evaluation of the model you intend to use, and by evaluating multiple models. | Measurement Bias. This bias is introduced as the raw data is collected. It can be because of a faulty sensor, it can be because someone read a thermometer wrong, etc. Measurement bias can be difficult to eliminate entirely, but through careful experimental and equipment setup it can be minimized. | Sampling Bias. This is what happens when the data sample that has been used to train the model isn&#39;t representative of the actual data typically observed for the system. To avoid this, we can train and validate on multiple data samples collected under various conditions to ensure we have a broad enough training data set. | . That doesn&#39;t even begin to cover all the ways that bias can creep into your model, but it gives you an idea as to the kind of things you should be looking out for. . Variance is sort of the yin to bias&#39; yang. Where the bias of a system is caused by inadvertently fitting the model to unreal circumstances, variance is caused by entirely real fluctuations within a dataset. Our model of choice can end up being fit to the noise in the dataset, resulting in a model that can&#39;t really predict anything. . Bias and variance can both result in a model being a poor predictor, but it is impossible to eliminate either entirely. In fact, variance can be helpful in reducing bias by introducing random variation to the training data. At the same time, bias can be useful in reducing variance because it can enable the model to ignore the noise in the dataset. The relationship between bias and variance is a balancing act, and its important to getting any use out of a machine learning model. . How does model complexity tie in? . The complexity of a model is directly tied to the model bias discussed above, and we can illustrate that here. I&#39;ll be using the monthly sunspot dataset from machine learning mastery. Below I import the data, then render a table and plot to show what the data looks like. Note that the dates have been converted to Unix epoch time for the sake of simplicity. . &#39;&#39;&#39;First we import all the packages we&#39;ll be using&#39;&#39;&#39; import pandas as pd import numpy as np import datetime as dt from scipy.optimize import curve_fit import plotly.express as px # for visualization import plotly.graph_objs as go from plotly.figure_factory import create_table &#39;&#39;&#39;Here we import our data, and take a chunk of it for use in our analysis&#39;&#39;&#39; rawDataDF = pd.read_csv(&quot;monthly-sunspots.txt&quot;) rawDataDF[&quot;Epoch&quot;] = (pd.to_datetime(rawDataDF[&#39;Month&#39;]) - dt.datetime(1970,1,1)).dt.total_seconds() df = rawDataDF.iloc[:151,:] table = create_table(rawDataDF.head()) table.show() fig = px.scatter(df, x=&#39;Epoch&#39;, y=&#39;Sunspots&#39;) fig.show() . . . . . We can use scipy to generate a really simple linear model for the data. This is a pretty poor fit for the data sample, but we don&#39;t expect that much from a linear model. It doesn&#39;t have enough complexity to capture the actual shape of the data. . &#39;&#39;&#39;This function is for use with scipy&#39;s curve_fit, seen below&#39;&#39;&#39; def func(x,b,m): return m*x + b &#39;&#39;&#39;We perform the fit, and store the result in our dataframe alongside the raw data.&#39;&#39;&#39; popt, pcov = curve_fit(func, df[&#39;Epoch&#39;],df[&#39;Sunspots&#39;]) df[&#39;simpleFit&#39;] = df[&#39;Epoch&#39;].apply(lambda x: func(x,popt[0],popt[1])) fig.add_scatter(x=df[&#39;Epoch&#39;], y=df[&#39;simpleFit&#39;], mode=&#39;lines&#39;) fig.show() . . . By adding another term to this equation, making it a quadratic, we can get a slightly better fit. . &#39;&#39;&#39;This code cell is similar to the above one, with a slightly more complex fit.&#39;&#39;&#39; def func(x,b,m,a): return a*(x**2) + m*x + b popt, pcov = curve_fit(func, df[&#39;Epoch&#39;],df[&#39;Sunspots&#39;]) df[&#39;simpleFit&#39;] = df[&#39;Epoch&#39;].apply(lambda x: func(x,popt[0],popt[1],popt[2])) fig.add_scatter(x=df[&#39;Epoch&#39;], y=df[&#39;simpleFit&#39;], mode=&#39;lines&#39;) fig.show() . . . In fact, according to Taylor&#39;s Theorem, it should be possible to get a very good estimation of this data by adding more terms. Below, you can see a plot with a slider that allows you to explore how an increasing number of parameters offer a better fit to the shown data. . &#39;&#39;&#39;This section contains code that dynamically generates functions with a given number of parameters, and fits them using scipy. You can take a look if you want, but understanding it isn&#39;t necessary for this discussion.&#39;&#39;&#39; def funcBuilder(numParams): result = [&quot;x&quot;] count = 0 for i in range(numParams): count = count + 1 result.append(&quot;,a&quot;+str(i)) funcStr = list(&quot;def func(&quot;) funcStr.extend(result) funcStr.extend(&quot;): n&quot;) funcStr.extend(&quot; result = 0&quot;) count = 0 for i in range(0,numParams): funcStr.extend(&quot;+ (x &quot;+ &quot;**&quot; + str(i) + &quot;)&quot; + &quot; * a&quot; + str(i) ) funcStr.extend(&quot; n return result&quot;) funcStr = &quot;&quot;.join(funcStr) return funcStr poptList = [] popt = [] for numParams in range(1,15,1): exec(funcBuilder(numParams)) popt, pcov = curve_fit(func, df[&#39;Epoch&#39;],df[&#39;Sunspots&#39;], p0 = np.append(popt,1)) poptList.append(popt) df[&#39;fit&#39;+str(numParams)] = df[&#39;Epoch&#39;].apply(lambda x: func(x, *popt)) fig = px.scatter(df, x=&#39;Epoch&#39;, y=&#39;Sunspots&#39;) fitCols = [x for x in df.columns if &quot;fit&quot; in x] steps = [] for col in fitCols: fig.add_trace( go.Scatter( visible=False, x=df[&quot;Epoch&quot;], y=df[col] ) ) fig.data[0].visible = True for i in range(len(fig.data)): numParams = dict( method=&quot;update&quot;, args=[{&quot;visible&quot;: [False] * len(fig.data), &quot;showlegend&quot;:False}], # layout attribute label=str(i) ) numParams[&quot;args&quot;][0][&quot;visible&quot;][0] = True numParams[&quot;args&quot;][0][&quot;visible&quot;][i] = True # Toggle i&#39;th trace to &quot;visible&quot; steps.append(numParams) sliders = [dict( active=0, currentvalue={&quot;prefix&quot;: &quot;Number of terms: &quot;}, pad={&quot;t&quot;: 50}, steps=steps )] fig.layout.sliders = sliders fig.show() . . . . This next piece of code calculates the Mean Absolute Percent Error (MAPE) for the fits. A lower value here represents a better fit. This shows that, despite increasing the complexity of the model, four parameters offers the best fit for the data. . &#39;&#39;&#39;We get all the columns with &quot;fit&quot; in the title and use them to calculate the MAPE for our fits.&#39;&#39;&#39; fitCols = [x for x in df.columns if &quot;fit&quot; in x] dfAPE = pd.DataFrame() dfMAPE = [] for col in fitCols: dfAPE[col+&quot;AbsErr&quot;] = df.apply(lambda x: 0 if x[&quot;Sunspots&quot;] == 0.0 else abs(x[col] - x[&quot;Sunspots&quot;])/x[&quot;Sunspots&quot;],axis=1) dfMAPE.append([int(col.split(&quot;t&quot;)[-1]),dfAPE[col+&quot;AbsErr&quot;].iloc[-1]/len(dfAPE[col+&quot;AbsErr&quot;])]) dfMAPE1 = pd.DataFrame(dfMAPE, columns=[&quot;numParams&quot;,&quot;MAPE&quot;]) fig = px.scatter(dfMAPE1, x=&#39;numParams&#39;, y=&#39;MAPE&#39;) fig.show() . . . Those results are actually kind of misleading though. In the plot above, even the poor fits still have a percent error less than one, but let&#39;s see what happens when we explore another subset of the data. . &#39;&#39;&#39;Here we grab the next 150 points of data and plot them.&#39;&#39;&#39; df = rawDataDF.iloc[150:301,:] fig = px.scatter(df, x=&#39;Epoch&#39;, y=&#39;Sunspots&#39;) fig.show() . . . Here we will plot our previous model fits using our new data sample. Explore how adding more parameters affects the fit of this data. . &#39;&#39;&#39;This is another chunk of code that is sort of complex, and not strictly necessary for understanding the larger point.&#39;&#39;&#39; p0 = [] popt = [] for numParams in range(1,15,1): exec(funcBuilder(numParams)) df[&#39;fit&#39;+str(numParams)] = df[&#39;Epoch&#39;].apply(lambda x: func(x, *poptList[numParams-1])) fig = px.scatter(df, x=&#39;Epoch&#39;, y=&#39;Sunspots&#39;) fitCols = [x for x in df.columns if &quot;fit&quot; in x] steps = [] for col in fitCols: fig.add_trace( go.Scatter( visible=False, x=df[&quot;Epoch&quot;], y=df[col] ) ) fig.data[0].visible = True for i in range(len(fig.data)): numParams = dict( method=&quot;update&quot;, args=[{&quot;visible&quot;: [False] * len(fig.data), &quot;showlegend&quot;:False}], # layout attribute label=str(i) ) numParams[&quot;args&quot;][0][&quot;visible&quot;][0] = True numParams[&quot;args&quot;][0][&quot;visible&quot;][i] = True # Toggle i&#39;th trace to &quot;visible&quot; steps.append(numParams) sliders = [dict( active=0, currentvalue={&quot;prefix&quot;: &quot;Number of terms: &quot;}, pad={&quot;t&quot;: 50}, steps=steps )] fig.layout.sliders = sliders fig.show() . . . . These fits are terrible! What does the MAPE look like? . &#39;&#39;&#39;Calculating the MAPE the same way we did previously.&#39;&#39;&#39; fitCols = [x for x in df.columns if &quot;fit&quot; in x] dfAPE = pd.DataFrame() dfMAPE = [] for col in fitCols: dfAPE[col+&quot;AbsErr&quot;] = df.apply(lambda x: 0 if x[&quot;Sunspots&quot;] == 0.0 else abs(x[col] - x[&quot;Sunspots&quot;])/x[&quot;Sunspots&quot;],axis=1) dfMAPE.append([int(col.split(&quot;t&quot;)[-1]),dfAPE[col+&quot;AbsErr&quot;].iloc[-1]/len(dfAPE[col+&quot;AbsErr&quot;])]) dfMAPE2 = pd.DataFrame(dfMAPE, columns=[&quot;numParams&quot;,&quot;MAPE&quot;]) fig = px.scatter(dfMAPE2, x=&#39;numParams&#39;, y=&#39;MAPE&#39;) fig.show() . . . Notice the magnitude of the MAPE in the above plot. This is far worse than the fits on that first data sample. Let&#39;s overlay our MAPEs for a direct comparison. . &#39;&#39;&#39;Overlaying the MAPE plots for easy comparison. The y-axis had to be a log plot in order for them both to appear on the same plot. You know things have gotten bad when...&#39;&#39;&#39; fig = px.line(log_y=True,) fig.add_trace(go.Scatter(x=dfMAPE1[&quot;numParams&quot;],y=dfMAPE1[&quot;MAPE&quot;], legendgroup=&quot;MAPE1&quot;,name=&quot;MAPE1&quot;)) fig.add_trace(go.Scatter(x=dfMAPE2[&quot;numParams&quot;],y=dfMAPE2[&quot;MAPE&quot;], legendgroup=&quot;MAPE2&quot;,name=&quot;MAPE2&quot;)) fig.data[0][&quot;showlegend&quot;] = True fig.update_layout( xaxis_title=&quot;numParams&quot;, yaxis_title=&quot;MAPE&quot; ) fig.show() . . . In order to get them to show on the same plot, I had to put make the y-axis logarithmic. Yikes. It&#39;s incredibly clear that even though our model fit the original data set well, it doesn&#39;t appear to have any predicitve power for this application. . So what went wrong? . There are three major issues with how the above analysis was performed: . This model has no predictive power for this application. Using a polynomial fit can result in really nice curve fitting, and if we expected all new data point to appear on this line it might work, but this data is in a time series. New data points will be generated at positive time steps, which makes our fit worthless. | We used a fairly small sample subset to fit our model. In order for the model to anticipate all behaviors in the data set, it needs to have seen them all. | You can see from the comparison of the MAPEs above that adding complexity to the model made things far worse on the unseen data, not better. When adding complexity to a model it is important to do so in a very measured manner. Things like the F-Test or the Akaike Information Criterion can be used to determine if your added complexity is actually providing a better fit, or if you&#39;re just introducing model bias. | The other points might be sins, but this last one is a cardinal sin. We validated our tests on the exact same data that we used to fit our model, which is the fastest way to introduce bias to your system. In order to avoid this, we need to split the data into training, validation, and testing samples. We iteratively train the model on the training set, and check its performance on the validation set. Then, when we&#39;re satisfied with the performance, we test it one last time on the testing set. | Up next . In a future post I&#39;ll be taking this data set, and creating a model that actually has some predictive power to describe it&#39;s behavior. Until next time! .",
            "url": "https://blog.tbhinnovation.com/beginner/2022/01/20/biasAndVariance.html",
            "relUrl": "/beginner/2022/01/20/biasAndVariance.html",
            "date": " ‚Ä¢ Jan 20, 2022"
        }
        
    
  

  
  

  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://blog.tbhinnovation.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}