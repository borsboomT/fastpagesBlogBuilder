{
  
    
        "post0": {
            "title": "Data Science vs Software Engineering - A Salary Comparison",
            "content": "A common question I see online in the various places data scientists and software engineers meet to discuss how nerdy we are is &quot;who makes more money?&quot; I&#39;ve seen a lot of claims thrown around without too much evidence to really substantiate anything, which is odd considering data scientists are part of the conversation. In order to put the arguments to rest, I decided to do a little digging to see if I could put the arguments to rest once and for all. . Levels.fyi is a well known aggregator of salary data for the tech sector. Granted, using data from levels.fyi might not be the best sampling methodology because it only contains data from people that self-report their salaries, but it&#39;s what was easily available. At any rate, it should suffice for a blog post. . I built a VERY quick and dirty webscraper in order to collect relevant salary data from their website using Selenium and BeautifulSoup. The code for the webscraper can be viewed here. Note that for the purposes of this analysis the regions have been limited to the US. . During the scraping process I had been having some issues with a spotty internet connection, so while scraping the data I was frequently saving it to disk in case of any errors. I then aggregated all the data together by career track into the files that can be seen in this folder, using the script found here. All told I ended up scraping data for the following career tracks: . Data Scientist | Product Manager | Recruiter | Sales | Software Engineer | Software Engineering Manager | Technical Program Manager | . With the data collected, we can get to work analysing it! The first thing we have to do is import the relevant libraries, and explore the data for any outliers. My preferred tool for this is called pandas-profiling, and its use is showcased below. . import pandas as pd import numpy as np import plotly.express as px import plotly.graph_objects as go from pandas_profiling import ProfileReport DS_FILE_LOCATION = &quot;https://raw.githubusercontent.com/borsboomT/levels_scraper/main/data/DataScientist_completeCSV.csv&quot; PM_FILE_LOCATION = &quot;https://raw.githubusercontent.com/borsboomT/levels_scraper/main/data/ProductManager_completeCSV.csv&quot; REC_FILE_LOCATION = &quot;https://raw.githubusercontent.com/borsboomT/levels_scraper/main/data/Recruiter_completeCSV.csv&quot; SAL_FILE_LOCATION = &quot;https://raw.githubusercontent.com/borsboomT/levels_scraper/main/data/Sales_completeCSV.csv&quot; SWE_FILE_LOCATION = &quot;https://raw.githubusercontent.com/borsboomT/levels_scraper/main/data/SoftwareEngineer_completeCSV.csv&quot; SWEM_FILE_LOCATION = &quot;https://raw.githubusercontent.com/borsboomT/levels_scraper/main/data/SoftwareEngineeringManager_completeCSV.csv&quot; TPM_FILE_LOCATION = &quot;https://raw.githubusercontent.com/borsboomT/levels_scraper/main/data/TechnicalProgramManager_completeCSV.csv&quot; . raw_DS_df = pd.read_csv(DS_FILE_LOCATION) # This ensures that the numerical columns are interpreted as numeric columns by # converting all N/A values to NaN. numeric_cols = [ &quot;Total Compensation&quot;, &quot;Stock Comp&quot;, &quot;Base Comp&quot;, &quot;Bonus Comp&quot;, &quot;YOE Total&quot;, &quot;YOE At Company&quot;, ] raw_DS_df[numeric_cols] = raw_DS_df[numeric_cols].apply( pd.to_numeric, errors=&quot;coerce&quot; ) # We also round and convert the experience columns to integer values # for easier grouping later on. exp_cols = [&quot;YOE Total&quot;, &quot;YOE At Company&quot;] raw_DS_df[exp_cols] = raw_DS_df[exp_cols].apply(np.round) raw_DS_df[exp_cols] = raw_DS_df[exp_cols].astype(int) profile = ProfileReport(raw_DS_df, title=&quot;Pandas Profiling Report&quot;) profile.to_notebook_iframe() . This report gives us a good overview of the way our data looks. We can see the number of distinct values for categorical features, as well as the distribution of values for numerical features. By clicking the &quot;Toggle Details&quot; button for the numerical features we can also seem some general statistics about them. . Data Cleaning and Outlier Detection . This data clearly has some fairly substantial outliers. These data points may well be real, after all the salaries in tech are known for being quite high and somewhat erratic, but we&#39;re more interested in the data as it pertains to the average person. For this reason, we&#39;re going to remove the outliers using the interquartile range (IQR) method. We need to be careful though, as we expect different salaries depending on the number of years of experience someone has. That means we need to group the salaries by experience level before we do any data pruning. . YOE_vals = raw_DS_df[&quot;YOE Total&quot;].unique() no_outlier_df_list = [] for YOE in YOE_vals: # We filter the dataframe based on the total years of experience. df = raw_DS_df[raw_DS_df[&quot;YOE Total&quot;] == YOE] # We get the upper and lower quantile data for the numeric columns. lower_quant = df.quantile(0.25) upper_quant = df.quantile(0.75) IQR = upper_quant - lower_quant cut_off = IQR * 1.5 lower_cut_off, upper_cut_off = lower_quant - cut_off, upper_quant + cut_off # We filter the data based on those quantiles quant_filter = ( df[numeric_cols] &gt; (lower_cut_off)) &amp; ( df[numeric_cols] &lt; (upper_cut_off) ) df = df[quant_filter.any(axis=1)] # We save the filtered dataframe by adding it to a list no_outlier_df_list.append(df) # Once all the outliers have been filtered, we concatenate the filtered dataframes # back into a single dataframe. no_outliers_DS_df = pd.concat(no_outlier_df_list) no_outliers_DS_df.reset_index(inplace=True, drop=True) . We also have a lot of missing data points. Since we&#39;ll only really be using aggregates to look at this data, we can handle the missing data points using imputation. In other words, we&#39;ll replace the missing data with the median value for each feature. Again, we have to be careful by first grouping the data based on the total years of experience. Note: Here we&#39;re looping over the years of experience again, typically in order to make the process more efficient we would merge this task and the previous one into a single task so that we only have to loop through the data a single time. . imputed_df_list = [] for YOE in YOE_vals: # We filter the dataframe based on the total years of experience. df = no_outliers_DS_df[no_outliers_DS_df[&quot;YOE Total&quot;] == YOE] # We fill the NaN cells with the imputed median values df.fillna(df.median(), inplace=True) # We save the imputed dataframe by adding it to a list imputed_df_list.append(df) # Once all the outliers have been filtered, we concatenate the filtered dataframes # back into a single dataframe. imputed_DS_df = pd.concat(imputed_df_list) imputed_DS_df.reset_index(inplace=True, drop=True) . Lets check out the data to make sure everything is looking alright. . profile = ProfileReport(imputed_DS_df, title=&quot;Pandas Profiling Report&quot;) profile.to_notebook_iframe() . Much better! Since we have to do this for each individual career track dataset, we&#39;ll combine the process into a single callable function. . def clean_salary_data(raw_df): # This ensures that the numerical columns are interpreted as numeric columns by # converting all N/A values to NaN. numeric_cols = [ &quot;Total Compensation&quot;, &quot;Stock Comp&quot;, &quot;Base Comp&quot;, &quot;Bonus Comp&quot;, &quot;YOE Total&quot;, &quot;YOE At Company&quot;, ] raw_df[numeric_cols] = raw_df[numeric_cols].apply( pd.to_numeric, errors=&quot;coerce&quot; ) # We also round and convert the experience columns to integer values for easier grouping later on. exp_cols = [&quot;YOE Total&quot;, &quot;YOE At Company&quot;] raw_df.replace([np.inf, -np.inf], np.nan, inplace=True) raw_df.dropna(subset=exp_cols, how=&quot;all&quot;, inplace=True) raw_df[exp_cols] = raw_df[exp_cols].apply(np.round) raw_df[exp_cols] = raw_df[exp_cols].astype(int) YOE_vals = raw_df[&quot;YOE Total&quot;].unique() cleaned_df_list = [] for YOE in YOE_vals: # We filter the dataframe based on the total years of experience. df = raw_df[raw_df[&quot;YOE Total&quot;] == YOE] # We get the upper and lower quantile data for the numeric columns. lower_quant = df.quantile(0.25) upper_quant = df.quantile(0.75) IQR = upper_quant - lower_quant cut_off = IQR * 1.5 lower_cut_off, upper_cut_off = ( lower_quant - cut_off, upper_quant + cut_off, ) # We filter the data based on those quantiles quant_filter = (df[numeric_cols] &gt; (lower_cut_off)) &amp; ( df[numeric_cols] &lt; (upper_cut_off) ) df = df[quant_filter.any(axis=1)] # We fill the NaN cells with the imputed median values df.fillna(df.median(), inplace=True) # We save the cleaned dataframe by adding it to a list cleaned_df_list.append(df) cleaned_df = pd.concat(cleaned_df_list) cleaned_df.reset_index(inplace=True, drop=True) return cleaned_df . Alright, lets get the Data Scientist and Software Engineer data into the worksheet and clean it using our new function. . cleaned_DS_df = clean_salary_data(pd.read_csv(DS_FILE_LOCATION)) cleaned_SWE_df = clean_salary_data(pd.read_csv(SWE_FILE_LOCATION)) . So who makes more money? . Now that we have the relevant data, and it&#39;s reasonably clean, let&#39;s take a look at the total year compensation as a function of the number of years of experience for both DS and SWE employees. Unfortunately, there isn&#39;t much data for DS beyond the 10 year mark, so that&#39;s the maximum year we will take our comparison to. . tot_years_DS_df = cleaned_DS_df.groupby([&quot;YOE Total&quot;]).agg( [&quot;mean&quot;, &quot;count&quot;, &quot;std&quot;, &quot;median&quot;] ) tot_years_SWE_df = cleaned_SWE_df.groupby([&quot;YOE Total&quot;]).agg( [&quot;mean&quot;, &quot;count&quot;, &quot;std&quot;, &quot;median&quot;] ) fig = go.Figure() fig.add_trace( go.Scatter( name=&quot;DS_med&quot;, x=tot_years_DS_df.index, y=tot_years_DS_df[&quot;Total Compensation&quot;][&quot;median&quot;], mode=&quot;markers&quot;, ) ) fig.add_trace( go.Scatter( name=&quot;SWE_med&quot;, x=tot_years_SWE_df.index, y=tot_years_SWE_df[&quot;Total Compensation&quot;][&quot;median&quot;], mode=&quot;markers&quot;, ) ) fig.update_layout( xaxis_title=&quot;Total Years of Experience&quot;, yaxis_title=&quot;Total Yearly Compensation (USD)&quot;, xaxis_range=[-0.5, 10.5], yaxis_range=[150000, 350000], ) fig.update_layout(showlegend=True) fig.show() . . . Based on this comparison it appears that SWE do make slightly more than DS, the median values are clearly somewhat higher for the SWE. Let&#39;s quantify that difference using a 95% confidence interval. . salary_diff_DS_SWE = ( tot_years_SWE_df[&quot;Total Compensation&quot;][&quot;median&quot;].iloc[:11] - tot_years_DS_df[&quot;Total Compensation&quot;][&quot;median&quot;].iloc[:11] ) count_diff = salary_diff_DS_SWE.count() mean_diff, std_diff = salary_diff_DS_SWE.mean(), salary_diff_DS_SWE.std() conf_interval = 1.96 * std_diff / np.sqrt(count_diff) print( &quot;Estimated Income Difference Between DS and SWE = {} +/- {}&quot;.format( round(mean_diff, 2), round(conf_interval, 2) ) ) . Estimated Income Difference Between DS and SWE = 1681.82 +/- 3949.45 . This illustrates that while it does appears that SWE make more money than DS given the same number of years of experience, a difference of $0 is within the 95% confidence interval indicating that we cannot necessarily say that for certain. Ultimately, the median income of both career paths is quite similar and if you have decided to take either path you&#39;re going to end up doing just fine. At the end of the day you need to find something you enjoy doing, and worry less about what appears to be a marginal difference in salary at best. . Up Next . Ever consider the difference between job hopping and employer tenure? A career as a recruiter? A big move to the Bay Area? Now that we have a nice function for cleaning this data up, we&#39;ll be doing a more thorough analysis of the factors that impact salary. Maybe we&#39;ll be able to answer a few of your burning questions. .",
            "url": "https://blog.tbhinnovation.com/ds-swe-salary-comparison/",
            "relUrl": "/ds-swe-salary-comparison/",
            "date": " â€¢ May 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Optimize ARIMA Model Fits",
            "content": ". My last post ended with a reasonably successful fit of some time series data using the ARIMA model. To start this post off, I&#39;ve applied the exact same method to fit a different dataset. The dataset we used in the previous post was pretty small, and didn&#39;t leave many options for train/test protocols. This new dataset is quite a bit longer, just to help illustrate how some of these techniques work. The dataset we&#39;ll be using is the monthly sunspot dataset from Jason Brownlee&#39;s GitHub. The resulting fit from using the method found in my previous post is shown below. . . . That looks to be a pretty good fit, recall that ARIMA is only intended for short-term forecasting, but we haven&#39;t actually quantified how good the fit is in any way. In order to do that, we typically use something called the Root Mean Squared Error (RMSE). . Root Mean Squared Error . The RMSE of a fit is determined according to the following formula: . $RMSE = sqrt{ frac{1}{n} sum_{i=1}^{n} big(Y_i - hat{Y}_i big)^2}$ . Where $Y_i$ represents the experimental data, $ hat{Y}_i$ represents the model estimations. This parameter should only be calculated for the portion of the data that the model hasn&#39;t seen, also called the validation data set. So, let&#39;s grab that data set again, fit it the same way as last time, and do some math. . import pandas as pd import plotly.express as px # for visualization import plotly.graph_objects as go from statsmodels.tsa.arima.model import ARIMA from plotly.subplots import make_subplots # We load the data into a pandas DataFrame raw_data_DF = pd.read_csv(&quot;monthly-sunspots.txt&quot;, parse_dates=[0]) # Then we split it into training and test sets train_set_size = int(len(raw_data_DF.index) * 0.66) df = raw_data_DF.iloc[:train_set_size] timeSeries = df[&#39;Sunspots&#39;] # These are the ideal ARIMA fit parameters determined according to the method from my previous post pVal = 37 qVal = 6 dVal = 0 # Then we fit the model. model = ARIMA(timeSeries, order=(pVal,dVal,qVal)) result = model.fit() . df = raw_data_DF timeSeries = df[&#39;Sunspots&#39;] predictedVals = result.get_prediction(start=0,end=len(timeSeries)) df[&#39;estimate&#39;] = predictedVals.summary_frame()[&#39;mean&#39;] # And we calculate the RMSE from the portion of validation portion of the DataFrame validationDF = df.iloc[train_set_size:] RMSE = (((validationDF[&quot;Sunspots&quot;] - validationDF[&quot;estimate&quot;]) ** 2).mean() ) ** 0.5 print(&quot;RMSE = &quot;, RMSE) . RMSE = 52.377481932252316 . The RMSE is just over 50. Is that good? Is it bad? The RMSE is convenient because its raw value is in the same units as the value we are predicting, which makes it easy to visualize the amount of error in our predictions. That makes it clear that for this metric lower is better, but we don&#39;t have anything to compare it to. We&#39;re also not totally certain that we&#39;ve chosen the best parameters for the model, and we&#39;ve only trained and tested the model on one dataset. That&#39;s a lot of uncertainty, and if we want to put something like this into production we&#39;re going to have to do better. . Walk Forward Validation . It would be helpful if we had more datasets to train and validate this data on, but data can be difficult and expensive to collect. Gathering enough data to make a decent model can also take a lot of time. That&#39;s where Walk Forward Validation comes in. Previously, we split our dataset into a single training and validation dataset. . . . If we get a little bit smarter though, we can split it into multiple sets, treating each individual subset as an entirely different data set. We have to be careful when doing this with time series data, and make sure that no information from the future leaks into the past when we&#39;re training the model. No one wants another Back to the Future 3. Before we do anything though, lets save the last 10% of our dataset for use in a final test to check the quality of our fit. . from plotly.subplots import make_subplots # We hold back 10% of the data for use in a final validation holdback_DF_size = int(len(raw_data_DF.index) * 0.10) walk_forward_DF = raw_data_DF[:-holdback_DF_size] # This is a fun bit of code that determines the size of the training and validation sets necessary to break our dataset into # num_data_sets different datasets. num_data_sets = 6 validation_set_size = int(len(walk_forward_DF.index)/(num_data_sets+2)) train_set_size = validation_set_size * 2 fig = make_subplots(rows=6, cols=1, shared_xaxes=True) # We walk through the data, and break it up into num_data_sets individual train/test splits. # The resulting splits have been plotted below. train_set_start = 0 train_set_end = train_set_start + train_set_size validation_set_start = train_set_end validation_set_end = validation_set_start + validation_set_size data_set_number = 1 while validation_set_end &lt; len(walk_forward_DF.index): df_train = walk_forward_DF.iloc[train_set_start:train_set_end] df_valid = walk_forward_DF.iloc[validation_set_start:validation_set_end] fig.add_trace( go.Scatter( name= &quot;Training Set&quot;, legendgroup=&quot;group&quot; + str(data_set_number), legendgrouptitle_text=str(data_set_number), x=df_train[&#39;Month&#39;], y=df_train[&#39;Sunspots&#39;], mode=&#39;lines&#39;), row=data_set_number, col=1 ) fig.add_trace( go.Scatter( name= &quot;Validation Set&quot;, legendgroup=&quot;group&quot; + str(data_set_number), legendgrouptitle_text=str(data_set_number), x=df_valid[&#39;Month&#39;], y=df_valid[&#39;Sunspots&#39;], mode=&#39;lines&#39;), row=data_set_number, col=1 ) train_set_start = train_set_start + validation_set_size train_set_end = train_set_start + train_set_size validation_set_start = train_set_end validation_set_end = validation_set_start + validation_set_size data_set_number = data_set_number + 1 fig.update_xaxes(title_text=&quot;Date&quot;, row=6,col=1) fig.update_layout(height=800) fig.show() . . . Now we have six individual datasets to train and test on! This is a pretty small dataset, and we want to make sure each individual dataset is large enough to be useful, so we can&#39;t go too much smaller than that. In practice, a dataset can be broken into as many or as few test/validation pairs as you need so long as you don&#39;t make them too small! . Let&#39;s use this in combination with the RMSE we just learned about to get a better sense of how our model is doing! Remember, we need to fit on each training set as we loop through the data and then determine the RMSE from the validation set. . pVal = 37 qVal = 6 dVal = 0 fig = make_subplots(rows=6, cols=1, shared_xaxes=True) RMSE_list = [] num_data_sets = 6 validation_set_size = int(len(walk_forward_DF.index)/(num_data_sets+2)) train_set_size = validation_set_size * 2 # Basically the same code as above, but this time we fit our ARIMA model on each training set and test it on the validation set. # The resulting fits have been plotted alongside the train/test splits shown previously. train_set_start = 0 train_set_end = train_set_start + train_set_size validation_set_start = train_set_end validation_set_end = validation_set_start + validation_set_size data_set_number = 1 while validation_set_end &lt; len(walk_forward_DF.index): df = walk_forward_DF.iloc[train_set_start:validation_set_end] df_train = walk_forward_DF.iloc[train_set_start:train_set_end] df_valid = walk_forward_DF.iloc[validation_set_start:validation_set_end] train_time_series = df_train[&#39;Sunspots&#39;] model = ARIMA(train_time_series, order=(pVal,dVal,qVal)) result = model.fit() predictedVals = result.get_prediction(start=0,end=validation_set_end - train_set_start) df[&#39;estimate&#39;] = predictedVals.summary_frame()[&#39;mean&#39;] validationDF = df.iloc[train_set_size:] RMSE = (((validationDF[&quot;Sunspots&quot;] - validationDF[&quot;estimate&quot;]) ** 2).mean() ) ** 0.5 RMSE_list.append(RMSE) fig.add_trace( go.Scatter( name= &quot;Training Set&quot;, legendgroup=&quot;group&quot; + str(data_set_number), legendgrouptitle_text=str(data_set_number), x=df_train[&#39;Month&#39;], y=df_train[&#39;Sunspots&#39;], mode=&#39;lines&#39;), row=data_set_number, col=1 ) fig.add_trace( go.Scatter( name= &quot;Validation Set&quot;, legendgroup=&quot;group&quot; + str(data_set_number), legendgrouptitle_text=str(data_set_number), x=df_valid[&#39;Month&#39;], y=df_valid[&#39;Sunspots&#39;], mode=&#39;lines&#39;), row=data_set_number, col=1 ) fig.add_trace( go.Scatter( name= &quot;Model Prediction&quot;, legendgroup=&quot;group&quot; + str(data_set_number), legendgrouptitle_text=str(data_set_number), x=df[&#39;Month&#39;], y=df[&#39;estimate&#39;], mode=&#39;lines&#39;), row=data_set_number, col=1 ) train_set_start = train_set_start + validation_set_size train_set_end = train_set_start + train_set_size validation_set_start = train_set_end validation_set_end = validation_set_start + validation_set_size data_set_number = data_set_number + 1 # Since we have a sample of fitting results, we can calculate a confidence interval for the RMSE values mean_RMSE = sum(RMSE_list)/len(RMSE_list) z_val = 1.96 # This is the z value for a 95% confidence interval conf_interval = z_val * np.std(RMSE_list)/np.sqrt(len(RMSE_list)) fig.update_xaxes(title_text=&quot;Date&quot;, row=6,col=1) fig.update_layout(height=800) fig.show() print(&quot;RMSE: &quot;, mean_RMSE, &quot; +/- &quot;, conf_interval) . . . RMSE: 41.16820390420083 +/- 11.071244210278234 . The confidence interval we see above, calculated using the various walk forward train/test splits we just made, gives us a better idea of our model performance for this data set. Because it gives us a range of values that we might be able to expect for the RMSE, we can evaluate for ourselves if the amount of error in that range is acceptable for whatever application we have in mind. The model fits for each of the train/test splits are also shown, to illustrate how this value describes the error in the fits. . If we&#39;re willing to commit to a very long computation, we can take our fitting a step further. . Grid Search and Walk Forward for ARIMA Optimization . To make our parameter estimation even more robust, we can perform a grid search of the parameters surrounding our best estimates. We perform a walk forward train/test validation of all the different parameter combinations, and record their mean RMSE values. Once a walk forward validation has been performed using each parameter combination, we take the paramater combination with the best mean RMSE. . holdback_DF_size = int(len(raw_data_DF.index) * 0.10) walk_forward_DF = raw_data_DF[:-holdback_DF_size] num_data_sets = 6 validation_set_size = int(len(walk_forward_DF.index)/(num_data_sets+2)) train_set_size = validation_set_size * 2 RMSE_results = dict() #This describes the values we will be testing for our ARIMA model for pVal in range (34,41): for qVal in range(4,9): for dVal in range(0,2): RMSE_list = [] train_set_start = 0 train_set_end = train_set_start + train_set_size validation_set_start = train_set_end validation_set_end = validation_set_start + validation_set_size while validation_set_end &lt; len(walk_forward_DF.index): df = walk_forward_DF.iloc[train_set_start:validation_set_end] df_train = walk_forward_DF.iloc[train_set_start:train_set_end] df_valid = walk_forward_DF.iloc[validation_set_start:validation_set_end] model = ARIMA(df_train[&#39;Sunspots&#39;], order=(pVal,dVal,qVal)) result = model.fit() predictedVals = result.get_prediction(start=0,end=validation_set_end - train_set_start) df[&#39;estimate&#39;] = predictedVals.summary_frame()[&#39;mean&#39;] validationDF = df.iloc[train_set_size:] RMSE = (((validationDF[&quot;Sunspots&quot;] - validationDF[&quot;estimate&quot;]) ** 2).mean() ) ** 0.5 RMSE_list.append(RMSE) train_set_start = train_set_start + validation_set_size train_set_end = train_set_start + train_set_size validation_set_start = train_set_end validation_set_end = validation_set_start + validation_set_size mean_RMSE = sum(RMSE_list)/len(RMSE_list) RMSE_results[mean_RMSE] = [pVal,dVal,qVal] min_key = np.inf for key in RMSE_results: if key &lt; min_key: min_key = key print(&quot;Best RMSE: &quot;, min_key) print(&quot;Best p,d,q: &quot;, RMSE_results[min_key]) . Best RMSE: 39.41639690453025 Best p,d,q: [36, 1, 5] . Grid Search Results . The above test determined that our optimum parameters are p = 36, d = 1, and q = 5. It gave us a slightly better RMSE, but that calculation took about 18 hours. Let&#39;s use our new optimally determined parameters with the walk forward method, and generate a distribution for the RMSE in using the new parameters. In principle this could have been done as part of the previous step, but for the sake of illustrating the process we&#39;ll do it here. . pVal = 36 qVal = 5 dVal = 1 holdback_DF_size = int(len(raw_data_DF.index) * 0.10) walk_forward_DF = raw_data_DF[:-holdback_DF_size] fig = make_subplots(rows=6, cols=1, shared_xaxes=True) RMSE_list = [] num_data_sets = 6 validation_set_size = int(len(walk_forward_DF.index)/(num_data_sets+2)) train_set_size = validation_set_size * 2 # Basically the same code as above, but this time we fit our ARIMA model on each training set and test it on the validation set. # The resulting fits have been plotted alongside the train/test splits shown previously. train_set_start = 0 train_set_end = train_set_start + train_set_size validation_set_start = train_set_end validation_set_end = validation_set_start + validation_set_size data_set_number = 1 while validation_set_end &lt; len(walk_forward_DF.index): df = walk_forward_DF.iloc[train_set_start:validation_set_end] df_train = walk_forward_DF.iloc[train_set_start:train_set_end] df_valid = walk_forward_DF.iloc[validation_set_start:validation_set_end] train_time_series = df_train[&#39;Sunspots&#39;] model = ARIMA(train_time_series, order=(pVal,dVal,qVal)) result = model.fit() predictedVals = result.get_prediction(start=0,end=validation_set_end - train_set_start) df[&#39;estimate&#39;] = predictedVals.summary_frame()[&#39;mean&#39;] validationDF = df.iloc[train_set_size:] RMSE = (((validationDF[&quot;Sunspots&quot;] - validationDF[&quot;estimate&quot;]) ** 2).mean() ) ** 0.5 RMSE_list.append(RMSE) fig.add_trace( go.Scatter( name= &quot;Training Set&quot;, legendgroup=&quot;group&quot; + str(data_set_number), legendgrouptitle_text=str(data_set_number), x=df_train[&#39;Month&#39;], y=df_train[&#39;Sunspots&#39;], mode=&#39;lines&#39;), row=data_set_number, col=1 ) fig.add_trace( go.Scatter( name= &quot;Validation Set&quot;, legendgroup=&quot;group&quot; + str(data_set_number), legendgrouptitle_text=str(data_set_number), x=df_valid[&#39;Month&#39;], y=df_valid[&#39;Sunspots&#39;], mode=&#39;lines&#39;), row=data_set_number, col=1 ) fig.add_trace( go.Scatter( name= &quot;Model Prediction&quot;, legendgroup=&quot;group&quot; + str(data_set_number), legendgrouptitle_text=str(data_set_number), x=df[&#39;Month&#39;], y=df[&#39;estimate&#39;], mode=&#39;lines&#39;), row=data_set_number, col=1 ) train_set_start = train_set_start + validation_set_size train_set_end = train_set_start + train_set_size validation_set_start = train_set_end validation_set_end = validation_set_start + validation_set_size data_set_number = data_set_number + 1 # Since we have a sample of fitting results, we can calculate a confidence interval for the RMSE values mean_RMSE = sum(RMSE_list)/len(RMSE_list) z_val = 1.96 # This is the z value for a 95% confidence interval conf_interval = z_val * np.std(RMSE_list)/np.sqrt(len(RMSE_list)) fig.update_xaxes(title_text=&quot;Date&quot;, row=6,col=1) fig.update_layout(height=800) fig.show() print(&quot;RMSE: &quot;, mean_RMSE, &quot; +/- &quot;, conf_interval) . . . RMSE: 39.41639690453025 +/- 11.408986555476803 . The confidence interval is about the same as our previous example. Let&#39;s compare the confidence intervals for our original and optimized models. . import plotly.graph_objects as go RMSE_original = 41.16820390420083 error_original = 11.071244210278234 RMSE_opt = 39.41639690453025 error_opt = 11.408986555476803 df = pd.DataFrame({ &quot;Method&quot;: [&quot;Original&quot;, &quot;Optimization&quot;], &quot;Value&quot;: [RMSE_original,RMSE_opt], &quot;Error&quot;: [error_original,error_opt] }) fig = go.Figure() fig.add_trace(go.Bar( x = df[&quot;Method&quot;], y = df[&quot;Value&quot;], error_y = dict( type = &#39;data&#39;, array = df[&quot;Error&quot;], visible = True ), )) fig.update_layout( title = &quot;Confidence interval comparison&quot;, width=400 ) fig.update_yaxes(title=&quot;RMSE&quot;) . . . So the actual means were approximately 41.2 for the original method, and 39.4 for the optimization. Those are almost negligibly different, especially when you consider that their confidence intervals are almost entirely overlapped. In order to directly compare the two fits, we should actually employ both RMSE and the Akaike Information Criterion (AIC). Recall that these should be calculated on data that the model has not been trained on. AIC essentially evaluates a models performance on a dataset while discouraging increased model complexity. Each model tested on the dataset will have a distinct AIC value, with lower values indicating better model performance. . There are many ways to calculate the AIC parameter of a fit, here we will use the residual sum of squares (RSS) as the likelihood parameter according to the following equations: . $AIC = 2k - 2 mathrm{ln}( hat{L})=2k - 2 mathrm{ln}(RSS)$ . $RSS = sum_{i=1}^{n} big(Y_i - hat{Y}_i big)^2$ . Where $k$ is the number of fitting parameters for the model, $ hat{L}$ is the likelihood function, $Y_i$ are the observed data points, and $ hat{Y_i}$ are the estimated data points. . pVal = 37 qVal = 6 dVal = 0 # Optimal model parameters pVal_opt = 36 qVal_opt = 5 dVal_opt = 1 # We hold back 10% of the data for use in a final validation holdback_DF_size = int(len(raw_data_DF.index) * 0.10) # Make a dataframe that doesn&#39;t include the holdback data df = raw_data_DF[:-holdback_DF_size] timeSeries = df[&#39;Sunspots&#39;] # Train both models on this data. model = ARIMA(timeSeries, order=(pVal,dVal,qVal)) result = model.fit() model_opt = ARIMA(timeSeries, order=(pVal_opt,dVal_opt,qVal_opt)) result_opt = model_opt.fit() # Make a dataframe that includes the holdback data df = raw_data_DF # Use both models to predict the holdback data predicted_vals = result.get_prediction(start=0,end=len(df.index)) predicted_vals_opt = result_opt.get_prediction(start=0,end=len(df.index)) df[&#39;estimate&#39;] = predicted_vals.summary_frame()[&#39;mean&#39;] df[&#39;estimate_opt&#39;] = predicted_vals_opt.summary_frame()[&#39;mean&#39;] # Make a dataframe from only the holdback data validation_DF = df[-holdback_DF_size:] # Determine the RMSE for the holdback portion, then plot the data alongside the model fits RMSE = (((validation_DF[&quot;Sunspots&quot;] - validation_DF[&quot;estimate&quot;]) ** 2).mean() ) ** 0.5 RMSE_opt = (((validation_DF[&quot;Sunspots&quot;] - validation_DF[&quot;estimate_opt&quot;]) ** 2).mean() ) ** 0.5 k_orig = pVal + qVal + 1 k_opt = pVal_opt + qVal_opt + 1 RSS_orig = sum((validation_DF[&quot;Sunspots&quot;] - validation_DF[&quot;estimate&quot;]) ** 2) RSS_opt = sum((validation_DF[&quot;Sunspots&quot;] - validation_DF[&quot;estimate_opt&quot;]) ** 2) AIC_orig = 2 * k_orig - 2 * np.log(RSS_orig) AIC_opt = 2 * k_opt - 2 * np.log(RSS_opt) fig = px.line() fig.add_trace(go.Scatter( name=&#39;Real Data&#39;, x=validation_DF[&#39;Month&#39;], y=validation_DF[&#39;Sunspots&#39;], mode=&#39;lines&#39; )) fig.add_trace(go.Scatter( name=&#39;Original Estimate&#39;, x=validation_DF[&#39;Month&#39;], y=validation_DF[&#39;estimate&#39;], mode=&#39;lines&#39; )) fig.add_trace(go.Scatter( name=&#39;Optimized Estimate&#39;, x=validation_DF[&#39;Month&#39;], y=validation_DF[&#39;estimate_opt&#39;], mode=&#39;lines&#39; )) fig.show() # Conveniently, statsmodels offers us a very simple way of determing the aic of our models. print(&quot;RMSE, AIC Original Model: &quot;, RMSE,AIC_orig) print(&quot;RMSE, AIC Optimized Model: &quot;, RMSE_opt,AIC_opt) . . . RMSE, AIC Original Model: 45.81924188577674 61.41736933251867 RMSE, AIC Optimized Model: 45.82037338502683 57.41727055434315 . The RMSE and AIC abserved when testing both our original and optimized models on our holdback data can be seen above. The RMSE for our original model is ever so slightly better than for our optimized model, with values of 45.819 and 45.820 respectively. The AIC has values of 61.4 for our original model and 57.4 for our optimized model, indicating that the optimal model is a better fit! This is because, while the original model may be offering a very slightly better fit in terms of absolute error, the optimized model has fewer fitting parameters. Adding extra fitting parameters with negligible model performance is discouraged using AIC, which results in the optimized model having a lower AIC value. . Finding the optimized model parameters ended up taking 18 hours though, and resulted in marginally better perfomance. It could be argued that, for this application, the original model is just as performant and without requiring a lengthy optimization. . Up Next . We&#39;ll either be taking a look at using Recurrent Neural Networks and Long Short-Term Memory to try to predict cryptocurrency prices, or using SHapley Additive exPlanations to define the most important input variables for complex machine learning models. .",
            "url": "https://blog.tbhinnovation.com/time-series-optimize-ARIMA/",
            "relUrl": "/time-series-optimize-ARIMA/",
            "date": " â€¢ Feb 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Time Series Analysis for Beginners",
            "content": ". In my previous post, we explored some common mistakes that people make when they first start picking up machine learning. In that post, we tried to fit some time series data using a polynomial and ended up with an incredibly poor predictor. One of the largest reasons for the poor performance seen in that post was the choice of model, in the case the polynomial, for making predictions. That was a rather extreme case; we had no reason to expect that a polynomial would have any predictive power for time series analysis. That makes for a very good example of what not to do: choose a model without understanding how or why it works. In this post we&#39;ll explore the Auto Regressive Integrated Moving Average (ARIMA) model for time series analysis. . Why do we care about ARIMA? . ARIMA is actually the combination of an Auto Regressive (AR) model and a Moving Average (MA) model, with the addition of the Integration (I) preprocessing step. We&#39;ll go over each of these in more detail in a moment. By combining these individual components, the resulting ARIMA model is quite robust. . ARIMA Pros . Great for short-term forecasts. | Single variable time series data is all that&#39;s required. | Works for data that isn&#39;t stationary (the mean of the data moves around). | . ARIMA Cons . Computationally intense. | Not great for long-term forecasting. | Doesn&#39;t work for seasonal data (for that we can use SARIMA) | . ARIMA offers a fairly good trade-off when analysing non-seasonal trending data for short-term forecasts. More complex models, like SARIMA mentioned above, can be introduced to take into account seasonality. VARMA can be used when multiple time series variables need to be predicted. There are tons of variations on the ARIMA structure, and each has its own use cases, but some of these models end up being very expensive to compute. ARIMA is also a great choice for practicing machine learning on simple datasets while you&#39;re still learning. . How ARIMA Works . Auto Regression (AR) . The AR part of ARIMA refers to the idea that, if the present data being observed in a time series dataset can be used to predict future values, we should be able to forecast future values by performing a regression from past data from the same dataset. An example of how the data would be arranged for this is shown below. . . . That&#39;s great, but how do we figure out what number of lag steps we should shift the data when trying to optimize an ARIMA model? That can be illustrated using something called an autocorrelation plot, which is described below. First, let&#39;s import a dataset and visualize it, but before we do anything we have an opportunity to avoid another major mistake showcased in my in my previous post. We&#39;re going to hold back some of our data from the model training, so that we can use it to validate the performance of the model on unseen data. The data we&#39;re using here is the monthly airline-passengers dataset from Jason Brownlee&#39;s GitHub. . import pandas as pd import plotly.express as px # for visualization &#39;&#39;&#39;Here we import our data, and take two thirds of it for model training&#39;&#39;&#39; raw_data_DF = pd.read_csv(&quot;airline-passengers.txt&quot;, parse_dates=[0]) train_set_size = int(len(raw_data_DF.index) * 0.66) df = raw_data_DF.iloc[:train_set_size] table = create_table(df.head()) table.show() fig = px.line(df, x=df[&#39;Month&#39;], y=df[&#39;Passengers&#39;]) fig.update_xaxes( dtick=&quot;M6&quot;, tickformat=&quot;%b n%Y&quot;) fig.show() . . . . . Next, we plot the autocorrelation of the data. This is can be done very conveniently using the &quot;statsmodels&quot; package in Python. . from statsmodels.graphics.tsaplots import plot_acf import matplotlib.pyplot as plt timeSeries = df[&#39;Passengers&#39;] fig = plot_acf(timeSeries, lags=60,alpha=None) axs = fig.axes axs[0].set_xlabel(&#39;Lags&#39;) plt.show() fig = plot_acf(timeSeries, lags=len(timeSeries)-1,alpha=None) axs = fig.axes axs[0].set_xlabel(&#39;Lags&#39;) plt.show() pVal = 30 . The above two plots are for the same data, but have different scales on the x-axis. What they describe is actually pretty intuitive! Essentially, each point on the plot estimates how correlated a data point is with a data point a few lags behind it. The number of lags is given by the x-axis, and the y-axis is a raw percentage. Based on this, we can estimate the number of lags we should use to optimize our ARIMA model. The first lag step at which this plot crosses the x-axis indicates the approximate number of lags that can contribute information for a regression, and makes for a good estimate for the lag difference in the model. This value is also called the p value for the ARIMA model, and here we estimate it to be about 30. . The AR portion of the model doesn&#39;t need to be used in all scenarios. . AR should be used when . The autocorrelation plot gently decays toward zero. | The partial autocorrelation, described below, quickyl jumps to zero. | . Moving Average (MA) . The MA for the ARIMA model describes the number of lag steps to be used for making a moving average window for the dataset. The average of the last k lag steps is taken, and the residuals between the lag steps and the moving average are used as additional datapoints for the ARIMA regression. The optimal q value, also called the Order of Moving Average, is chosen using a method similar to the p value we saw above. This time, we plot the partial autocorrelation of the data. . from statsmodels.graphics.tsaplots import plot_pacf fig = plot_pacf(timeSeries, lags=40,alpha=None) axs = fig.axes axs[0].set_xlabel(&#39;Lags&#39;) plt.show() qVal = 0 . This plot is quite similar to the autocorrelation plot we saw previously. It estimates the correlation between a datapoint and a point lagged k points behind it, but this time the correlation of all the intermediate points is removed. The estimate for the q value is given by the first lag step at which the partial autocorrelation plot crosses the x-axis. . Like the AR portion of the model, the MA portion doesn&#39;t need to be used in all scenarios. . MA should be used when . The model has a negative autocorrelation at lag = 1. | The autocorrelation drops off more sharply. | The partial autocorrelation decreases more gradually. | . Based on the above, we set the q value to 0 for this data set. . Integration (I) . The I portion of the model is what makes the ARIMA model capable of handling non-stationary (trending) data. It is done by differencing the data until the result appears stationary. Differencing is done by subtracting each data point by the value in the previous data point. The Pandas package in Python offers a very simple wayt to do this shown below, alongside plots of what the difference data looks like. . from statsmodels.tsa.stattools import adfuller from plotly.subplots import make_subplots import plotly.graph_objects as go fig = make_subplots(rows=3, cols=1, shared_xaxes=True) values = timeSeries.values result = adfuller(values) print(&#39;Raw Time Series p-Value: %f&#39; % result[1]) fig.add_trace( go.Scatter(name=&#39;Raw Time Series&#39;,x=timeSeries.index, y=timeSeries, mode=&#39;lines&#39;), row=1, col=1 ) diff = timeSeries.diff() diff = diff.dropna() values = diff.values result = adfuller(values) print(&#39;1 Diff Series p-Value: %f&#39; % result[1]) fig.add_trace( go.Scatter(name=&#39;1 Diff&#39;,x=diff.index, y=diff, mode=&#39;lines&#39;), row=2, col=1 ) diff = diff.diff() diff = diff.dropna() values = diff.values result = adfuller(values) print(&#39;2 Diff Series p-Value: %f&#39; % result[1]) fig.add_trace( go.Scatter(name=&#39;2 Diffs&#39;,x=diff.index, y=diff, mode=&#39;lines&#39;), row=3, col=1 ) fig.update_xaxes(title_text=&quot;Index&quot;, row=3,col=1) fig.show() dVal = 2 . Raw Time Series p-Value: 0.997389 1 Diff Series p-Value: 0.246882 2 Diff Series p-Value: 0.000000 . . . It is also very important that we don&#39;t over difference the data, as that can result in the loss of valuable information. Luckily, the Dickey-Fuller test allows us to test for stationary data and the statsmodels Python package offers us an easy means of implementing this test. We should test the raw data for stationarity before we perform any differencing, and continuously difference the data until we achieve a p-value of less than 0.05 from the Dickey-Fuller test. From the above test we can see that the optimal d value, also called the Degree of Differencing, for our dataset is approximately 2. . Fitting the Model . Now we have estimates for the required parameters for the ARIMA model, and we can try using it to fit and forecast our data. Again, the statsmodels Python package offers a convenient way to implement this model. . from statsmodels.tsa.arima.model import ARIMA model = ARIMA(timeSeries, order=(pVal,dVal,qVal)) result = model.fit() print(result.summary()) . SARIMAX Results ============================================================================== Dep. Variable: Passengers No. Observations: 95 Model: ARIMA(30, 2, 0) Log Likelihood -345.376 Date: Sun, 23 Jan 2022 AIC 752.751 Time: 22:16:02 BIC 831.262 Sample: 0 HQIC 784.452 - 95 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 -1.0233 0.148 -6.899 0.000 -1.314 -0.733 ar.L2 -1.0419 0.279 -3.734 0.000 -1.589 -0.495 ar.L3 -1.0792 0.377 -2.861 0.004 -1.819 -0.340 ar.L4 -1.2589 0.515 -2.444 0.015 -2.269 -0.249 ar.L5 -1.1113 0.636 -1.748 0.080 -2.357 0.135 ar.L6 -1.1484 0.703 -1.634 0.102 -2.526 0.229 ar.L7 -1.2290 0.798 -1.540 0.124 -2.793 0.335 ar.L8 -1.3655 0.883 -1.547 0.122 -3.096 0.365 ar.L9 -1.2579 0.887 -1.418 0.156 -2.996 0.481 ar.L10 -1.5027 0.918 -1.636 0.102 -3.303 0.297 ar.L11 -1.4445 1.015 -1.423 0.155 -3.434 0.545 ar.L12 -0.9065 1.045 -0.868 0.386 -2.955 1.142 ar.L13 -0.7989 1.026 -0.779 0.436 -2.809 1.211 ar.L14 -0.9890 0.998 -0.991 0.322 -2.946 0.968 ar.L15 -0.9129 0.972 -0.940 0.347 -2.817 0.992 ar.L16 -1.1499 0.889 -1.293 0.196 -2.893 0.593 ar.L17 -1.2720 0.787 -1.617 0.106 -2.814 0.270 ar.L18 -1.4537 0.768 -1.893 0.058 -2.959 0.051 ar.L19 -1.3773 0.792 -1.740 0.082 -2.929 0.174 ar.L20 -1.4658 0.726 -2.020 0.043 -2.888 -0.044 ar.L21 -1.3553 0.713 -1.900 0.057 -2.754 0.043 ar.L22 -1.3149 0.764 -1.720 0.085 -2.813 0.183 ar.L23 -1.1103 0.742 -1.496 0.135 -2.565 0.345 ar.L24 -0.9407 0.726 -1.295 0.195 -2.364 0.483 ar.L25 -0.7819 0.685 -1.141 0.254 -2.124 0.561 ar.L26 -0.8153 0.627 -1.300 0.194 -2.045 0.414 ar.L27 -0.6265 0.514 -1.218 0.223 -1.634 0.381 ar.L28 -0.4175 0.433 -0.965 0.334 -1.265 0.430 ar.L29 -0.2106 0.363 -0.580 0.562 -0.922 0.501 ar.L30 -0.2032 0.234 -0.867 0.386 -0.663 0.256 sigma2 68.2913 14.329 4.766 0.000 40.206 96.377 =================================================================================== Ljung-Box (L1) (Q): 0.77 Jarque-Bera (JB): 3.38 Prob(Q): 0.38 Prob(JB): 0.18 Heteroskedasticity (H): 2.06 Skew: 0.46 Prob(H) (two-sided): 0.05 Kurtosis: 3.20 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . Now, with the model fitted we can use it to attempt to forecast data that is unseen by the model. . df = raw_data_DF timeSeries = df[&#39;Passengers&#39;] predictedVals = result.get_prediction(start=0,end=len(timeSeries)) df[&#39;estimate&#39;] = predictedVals.summary_frame()[&#39;mean&#39;] fig = px.line() fig.add_trace(go.Scatter( name=&#39;Real Data&#39;, x=df[&#39;Month&#39;], y=df[&#39;Passengers&#39;], mode=&#39;lines&#39; )) fig.add_trace(go.Scatter( name=&#39;Model Estimate&#39;, x=df[&#39;Month&#39;], y=df[&#39;estimate&#39;], mode=&#39;lines&#39; )) fig.add_trace(go.Scatter( name=&#39;Train/Test Split&#39;, x=[df[&#39;Month&#39;][train_set_size],df[&#39;Month&#39;][train_set_size]], y=[0,600], line_width=3, line_dash=&quot;dash&quot;, line_color=&#39;black&#39; )) fig.update_xaxes( dtick=&quot;M6&quot;, tickformat=&quot;%b n%Y&quot;) fig.show() . . . This model works fairly well for forecasting the data! You can definitely see that as the forecast gets further from the end of the testing data more error is introduced. We won&#39;t quantify that right now, I&#39;ll handle that in my next post. . Up Next . Next time, we&#39;ll look at how we might quantify the error in the model forecast and how we can confirm whether or not the parameters we used for this model are actually the best we can do. Until next time! .",
            "url": "https://blog.tbhinnovation.com/time-series-intro-ARIMA/",
            "relUrl": "/time-series-intro-ARIMA/",
            "date": " â€¢ Jan 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Beginner Mistakes in Machine Learning",
            "content": ". So you&#39;ve decided to pick up machine learning. That&#39;s fantastic! It can be incredibly powerful, and open up a ton of opportunities. Before you get started, we should have a little talk about some of the most common mistakes that beginners make when learning machine learning. The biggest culprit of all: overfitting. . What is overfitting? . Using mathematical models to estimate different parameters and properties is nothing new. The concept is basically as old as math itself, and when used correctly it can be incredibly powerful (not to mention sexy.ðŸ˜‰) That has created a huge influx of people that want to learn how to get into the field of machine learning and become data scientists themselves. . One of the biggest roadblocks that people tend to run into when they are first learning how to use machine learning models is to make a model as complex as possible, and fit their training data to within an inch of its life. This is called overfitting, and it occurs when a model is fitted so well to a particular subset of data that it doesn&#39;t work on any other data at all. There are specific training and testing protocols for avoiding this, such as K-Fold Cross Validation, and I will explore how that works in another post. Here we&#39;ll focus on how overfitting occurs, and what it has to do with the bias and variance of a model. . Bias and variance . Bias actually describes any systematic error that can be introduced when fitting a model to a dataset. Common sources of bias include: . Model Bias. Bias introduced by choosing a model that is ill fit for the application at hand. You&#39;ll never be able to fit data well if the model you&#39;ve chosen is simply wrong for what you&#39;re doing. This can be eliminated through thoughtful evaluation of the model you intend to use, and by evaluating multiple models. | Measurement Bias. This bias is introduced as the raw data is collected. It can be because of a faulty sensor, it can be because someone read a thermometer wrong, etc. Measurement bias can be difficult to eliminate entirely, but through careful experimental and equipment setup it can be minimized. | Sampling Bias. This is what happens when the data sample that has been used to train the model isn&#39;t representative of the actual data typically observed for the system. To avoid this, we can train and validate on multiple data samples collected under various conditions to ensure we have a broad enough training data set. | . That doesn&#39;t even begin to cover all the ways that bias can creep into your model, but it gives you an idea as to the kind of things you should be looking out for. . Variance is sort of the yin to bias&#39; yang. Where the bias of a system is caused by inadvertently fitting the model to unreal circumstances, variance is caused by entirely real fluctuations within a dataset. Our model of choice can end up being fit to the noise in the dataset, resulting in a model that can&#39;t really predict anything. . Bias and variance can both result in a model being a poor predictor, but it is impossible to eliminate either entirely. In fact, variance can be helpful in reducing bias by introducing random variation to the training data. At the same time, bias can be useful in reducing variance because it can enable the model to ignore the noise in the dataset. The relationship between bias and variance is a balancing act, and its important to getting any use out of a machine learning model. . How does model complexity tie in? . The complexity of a model is directly tied to the model bias discussed above, and we can illustrate that here. I&#39;ll be using the monthly sunspot dataset from Jason Brownlee&#39;s Github. Below I import the data, then render a table and plot to show what the data looks like. Note that the dates have been converted to Unix epoch time for the sake of simplicity. . &#39;&#39;&#39;First we import all the packages we&#39;ll be using&#39;&#39;&#39; import pandas as pd import numpy as np import datetime as dt from scipy.optimize import curve_fit import plotly.express as px # for visualization import plotly.graph_objs as go from plotly.figure_factory import create_table &#39;&#39;&#39;Here we import our data, and take a chunk of it for use in our analysis&#39;&#39;&#39; rawDataDF = pd.read_csv(&quot;monthly-sunspots.txt&quot;) rawDataDF[&quot;Epoch&quot;] = (pd.to_datetime(rawDataDF[&#39;Month&#39;]) - dt.datetime(1970,1,1)).dt.total_seconds() df = rawDataDF.iloc[:151,:] table = create_table(rawDataDF.head()) table.show() fig = px.scatter(df, x=&#39;Epoch&#39;, y=&#39;Sunspots&#39;) fig.show() . . . . . We can use scipy to generate a really simple linear model for the data. This is a pretty poor fit for the data sample, but we don&#39;t expect that much from a linear model. It doesn&#39;t have enough complexity to capture the actual shape of the data. . &#39;&#39;&#39;This function is for use with scipy&#39;s curve_fit, seen below&#39;&#39;&#39; def func(x,b,m): return m*x + b &#39;&#39;&#39;We perform the fit, and store the result in our dataframe alongside the raw data.&#39;&#39;&#39; popt, pcov = curve_fit(func, df[&#39;Epoch&#39;],df[&#39;Sunspots&#39;]) df[&#39;simpleFit&#39;] = df[&#39;Epoch&#39;].apply(lambda x: func(x,popt[0],popt[1])) fig.add_scatter(x=df[&#39;Epoch&#39;], y=df[&#39;simpleFit&#39;], mode=&#39;lines&#39;) fig.show() . . . By adding another term to this equation, making it a quadratic, we can get a slightly better fit. . &#39;&#39;&#39;This code cell is similar to the above one, with a slightly more complex fit.&#39;&#39;&#39; def func(x,b,m,a): return a*(x**2) + m*x + b popt, pcov = curve_fit(func, df[&#39;Epoch&#39;],df[&#39;Sunspots&#39;]) df[&#39;simpleFit&#39;] = df[&#39;Epoch&#39;].apply(lambda x: func(x,popt[0],popt[1],popt[2])) fig.add_scatter(x=df[&#39;Epoch&#39;], y=df[&#39;simpleFit&#39;], mode=&#39;lines&#39;) fig.show() . . . In fact, according to Taylor&#39;s Theorem, it should be possible to get a very good estimation of this data by adding more terms. Below, you can see a plot with a slider that allows you to explore how an increasing number of parameters offer a better fit to the shown data. . &#39;&#39;&#39;This section contains code that dynamically generates functions with a given number of parameters, and fits them using scipy. You can take a look if you want, but understanding it isn&#39;t necessary for this discussion.&#39;&#39;&#39; def funcBuilder(numParams): result = [&quot;x&quot;] count = 0 for i in range(numParams): count = count + 1 result.append(&quot;,a&quot;+str(i)) funcStr = list(&quot;def func(&quot;) funcStr.extend(result) funcStr.extend(&quot;): n&quot;) funcStr.extend(&quot; result = 0&quot;) count = 0 for i in range(0,numParams): funcStr.extend(&quot;+ (x &quot;+ &quot;**&quot; + str(i) + &quot;)&quot; + &quot; * a&quot; + str(i) ) funcStr.extend(&quot; n return result&quot;) funcStr = &quot;&quot;.join(funcStr) return funcStr poptList = [] popt = [] for numParams in range(1,15,1): exec(funcBuilder(numParams)) popt, pcov = curve_fit(func, df[&#39;Epoch&#39;],df[&#39;Sunspots&#39;], p0 = np.append(popt,1)) poptList.append(popt) df[&#39;fit&#39;+str(numParams)] = df[&#39;Epoch&#39;].apply(lambda x: func(x, *popt)) fig = px.scatter(df, x=&#39;Epoch&#39;, y=&#39;Sunspots&#39;) fitCols = [x for x in df.columns if &quot;fit&quot; in x] steps = [] for col in fitCols: fig.add_trace( go.Scatter( visible=False, x=df[&quot;Epoch&quot;], y=df[col] ) ) fig.data[0].visible = True for i in range(len(fig.data)): numParams = dict( method=&quot;update&quot;, args=[{&quot;visible&quot;: [False] * len(fig.data), &quot;showlegend&quot;:False}], # layout attribute label=str(i) ) numParams[&quot;args&quot;][0][&quot;visible&quot;][0] = True numParams[&quot;args&quot;][0][&quot;visible&quot;][i] = True # Toggle i&#39;th trace to &quot;visible&quot; steps.append(numParams) sliders = [dict( active=0, currentvalue={&quot;prefix&quot;: &quot;Number of terms: &quot;}, pad={&quot;t&quot;: 50}, steps=steps )] fig.layout.sliders = sliders fig.show() . . . . This next piece of code calculates the Mean Absolute Percent Error (MAPE) for the fits. A lower value here represents a better fit. This shows that, despite increasing the complexity of the model, four parameters offers the best fit for the data. . &#39;&#39;&#39;We get all the columns with &quot;fit&quot; in the title and use them to calculate the MAPE for our fits.&#39;&#39;&#39; fitCols = [x for x in df.columns if &quot;fit&quot; in x] dfAPE = pd.DataFrame() dfMAPE = [] for col in fitCols: dfAPE[col+&quot;AbsErr&quot;] = df.apply(lambda x: 0 if x[&quot;Sunspots&quot;] == 0.0 else abs(x[col] - x[&quot;Sunspots&quot;])/x[&quot;Sunspots&quot;],axis=1) dfMAPE.append([int(col.split(&quot;t&quot;)[-1]),dfAPE[col+&quot;AbsErr&quot;].iloc[-1]/len(dfAPE[col+&quot;AbsErr&quot;])]) dfMAPE1 = pd.DataFrame(dfMAPE, columns=[&quot;numParams&quot;,&quot;MAPE&quot;]) fig = px.scatter(dfMAPE1, x=&#39;numParams&#39;, y=&#39;MAPE&#39;) fig.show() . . . Those results are actually kind of misleading though. In the plot above, even the poor fits still have a percent error less than one, but let&#39;s see what happens when we explore another subset of the data. . &#39;&#39;&#39;Here we grab the next 150 points of data and plot them.&#39;&#39;&#39; df = rawDataDF.iloc[150:301,:] fig = px.scatter(df, x=&#39;Epoch&#39;, y=&#39;Sunspots&#39;) fig.show() . . . Here we will plot our previous model fits using our new data sample. Explore how adding more parameters affects the fit of this data. . &#39;&#39;&#39;This is another chunk of code that is sort of complex, and not strictly necessary for understanding the larger point.&#39;&#39;&#39; p0 = [] popt = [] for numParams in range(1,15,1): exec(funcBuilder(numParams)) df[&#39;fit&#39;+str(numParams)] = df[&#39;Epoch&#39;].apply(lambda x: func(x, *poptList[numParams-1])) fig = px.scatter(df, x=&#39;Epoch&#39;, y=&#39;Sunspots&#39;) fitCols = [x for x in df.columns if &quot;fit&quot; in x] steps = [] for col in fitCols: fig.add_trace( go.Scatter( visible=False, x=df[&quot;Epoch&quot;], y=df[col] ) ) fig.data[0].visible = True for i in range(len(fig.data)): numParams = dict( method=&quot;update&quot;, args=[{&quot;visible&quot;: [False] * len(fig.data), &quot;showlegend&quot;:False}], # layout attribute label=str(i) ) numParams[&quot;args&quot;][0][&quot;visible&quot;][0] = True numParams[&quot;args&quot;][0][&quot;visible&quot;][i] = True # Toggle i&#39;th trace to &quot;visible&quot; steps.append(numParams) sliders = [dict( active=0, currentvalue={&quot;prefix&quot;: &quot;Number of terms: &quot;}, pad={&quot;t&quot;: 50}, steps=steps )] fig.layout.sliders = sliders fig.show() . . . . These fits are terrible! What does the MAPE look like? . &#39;&#39;&#39;Calculating the MAPE the same way we did previously.&#39;&#39;&#39; fitCols = [x for x in df.columns if &quot;fit&quot; in x] dfAPE = pd.DataFrame() dfMAPE = [] for col in fitCols: dfAPE[col+&quot;AbsErr&quot;] = df.apply(lambda x: 0 if x[&quot;Sunspots&quot;] == 0.0 else abs(x[col] - x[&quot;Sunspots&quot;])/x[&quot;Sunspots&quot;],axis=1) dfMAPE.append([int(col.split(&quot;t&quot;)[-1]),dfAPE[col+&quot;AbsErr&quot;].iloc[-1]/len(dfAPE[col+&quot;AbsErr&quot;])]) dfMAPE2 = pd.DataFrame(dfMAPE, columns=[&quot;numParams&quot;,&quot;MAPE&quot;]) fig = px.scatter(dfMAPE2, x=&#39;numParams&#39;, y=&#39;MAPE&#39;) fig.show() . . . Notice the magnitude of the MAPE in the above plot. This is far worse than the fits on that first data sample. Let&#39;s overlay our MAPEs for a direct comparison. . &#39;&#39;&#39;Overlaying the MAPE plots for easy comparison. The y-axis had to be a log plot in order for them both to appear on the same plot. You know things have gotten bad when...&#39;&#39;&#39; fig = px.line(log_y=True,) fig.add_trace(go.Scatter(x=dfMAPE1[&quot;numParams&quot;],y=dfMAPE1[&quot;MAPE&quot;], legendgroup=&quot;MAPE1&quot;,name=&quot;MAPE1&quot;)) fig.add_trace(go.Scatter(x=dfMAPE2[&quot;numParams&quot;],y=dfMAPE2[&quot;MAPE&quot;], legendgroup=&quot;MAPE2&quot;,name=&quot;MAPE2&quot;)) fig.data[0][&quot;showlegend&quot;] = True fig.update_layout( xaxis_title=&quot;numParams&quot;, yaxis_title=&quot;MAPE&quot; ) fig.show() . . . In order to get them to show on the same plot, I had to put make the y-axis logarithmic. Yikes. It&#39;s incredibly clear that even though our model fit the original data set well, it doesn&#39;t appear to have any predicitve power for this application. . So what went wrong? . There are three major issues with how the above analysis was performed: . This model has no predictive power for this application. Using a polynomial fit can result in really nice curve fitting, and if we expected all new data point to appear on this line it might work, but this data is in a time series. New data points will be generated at positive time steps, which makes our fit worthless. | We used a fairly small sample subset to fit our model. In order for the model to anticipate all behaviors in the data set, it needs to have seen them all. | You can see from the comparison of the MAPEs above that adding complexity to the model made things far worse on the unseen data, not better. When adding complexity to a model it is important to do so in a very measured manner. Things like the F-Test or the Akaike Information Criterion can be used to determine if your added complexity is actually providing a better fit, or if you&#39;re just introducing model bias. | The other points might be sins, but this last one is a cardinal sin. We validated our tests on the exact same data that we used to fit our model, which is the fastest way to introduce bias to your system. In order to avoid this, we need to split the data into training, validation, and testing samples. We iteratively train the model on the training set, and check its performance on the validation set. Then, when we&#39;re satisfied with the performance, we test it one last time on the testing set. | Up next . In my next post, I&#39;ll be exploring how to use the ARIMA model for time series analysis. Until next time! .",
            "url": "https://blog.tbhinnovation.com/beginner-mistakes/",
            "relUrl": "/beginner-mistakes/",
            "date": " â€¢ Jan 20, 2022"
        }
        
    
  

  
  

  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://blog.tbhinnovation.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}